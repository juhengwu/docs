---
title: "Monitoring"
description: "Track model health, detect drift, and respond quickly."
---

Monitoring keeps your AI system healthy after launch. It surfaces changes in quality and safety before users notice.

## What to monitor

- **Quality score** by evaluation and model version
- **Policy violations** and safety flags
- **Refusal rate** and false refusals
- **Latency and cost** signals tied to user experience
- **Drift** in input data or user intent

## Dashboards

Dashboards summarize your most important metrics and trends.

- Filter by time range, environment, or model version
- Slice metrics by customer segment or metadata
- Compare live results against your baseline

## Alerts

Set alerts to notify the team when thresholds are breached.

<Steps>
  <Step title="Choose a metric">
    Pick a critical evaluation or safety signal.
  </Step>
  <Step title="Set thresholds">
    Use a baseline run or policy limit to define alerts.
  </Step>
  <Step title="Route notifications">
    Notify Slack, email, or ticketing workflows.
  </Step>
</Steps>

## Drift detection

Drift can hide in subtle changes. Use these signals together:

- Shifts in input distributions
- Sudden score changes after a release
- New failure modes in error clusters

## Incident response

When an alert fires:

1. **Triaging**: Review the failing examples and metadata.
2. **Diagnosing**: Compare to baseline and staging runs.
3. **Mitigating**: Roll back, hotfix prompts, or update guardrails.
4. **Documenting**: Capture evidence and decisions for audit trails.

## Related docs

<CardGroup cols={2}>
  <Card title="Evaluation" icon="check-double" href="/evaluation">
    Build the signals you monitor in production.
  </Card>
  <Card title="Workspace" icon="folder" href="/workspace">
    Keep environments isolated and clean.
  </Card>
</CardGroup>
