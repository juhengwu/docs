---
title: "Get started"
description: "Create your first workspace, evaluation, and monitoring view."
---

This guide gets you from zero to your first evaluation run with Know Your AI.

<Info>
  **What you'll need**
  - A Know Your AI account
  - Access to your model inputs/outputs (logs, traces, or datasets)
  - A use case you want to measure (quality, safety, or compliance)
</Info>

## 1) Create a workspace

Workspaces keep data, evaluations, and access controls isolated by project.

<Steps>
  <Step title="Name the workspace">
    Choose a clear project name like “Customer Support Copilot”.
  </Step>
  <Step title="Add members">
    Invite teammates and assign roles (admin, editor, reviewer, viewer).
  </Step>
  <Step title="Set environments">
    Use dev, staging, and production environments to prevent accidental production changes.
  </Step>
</Steps>

## 2) Connect your data

Bring in recent model interactions from logs or CSV exports.

<Steps>
  <Step title="Create a dataset">
    Upload a CSV or connect a log stream from your application.
  </Step>
  <Step title="Map fields">
    Identify prompt, response, metadata, and optional ground truth fields.
  </Step>
  <Step title="Sample wisely">
    Start with a small representative slice to validate evaluation quality.
  </Step>
</Steps>

## 3) Build your first evaluation

Define what “good” looks like for your use case.

<Steps>
  <Step title="Pick evaluation type">
    Choose an automated metric, LLM-judge rubric, or human review.
  </Step>
  <Step title="Create a rubric">
    Add criteria like accuracy, policy adherence, tone, or grounding.
  </Step>
  <Step title="Set thresholds">
    Decide what passes vs. fails, and what requires review.
  </Step>
</Steps>

## 4) Run and review

Kick off a run and review results in the evaluation report.

- Compare scores across model versions
- Inspect failed examples
- Add notes for human review

## 5) Turn on monitoring

Once the evaluation looks good, schedule it to run regularly.

<Steps>
  <Step title="Schedule evaluations">
    Run daily or after every deployment.
  </Step>
  <Step title="Create alerts">
    Notify your team when quality falls below thresholds.
  </Step>
  <Step title="Share dashboards">
    Publish monitoring views to stakeholders.
  </Step>
</Steps>

## Next steps

<CardGroup cols={2}>
  <Card title="How to use it" icon="wand-magic" href="/how-to-use">
    Operational best practices and workflows.
  </Card>
  <Card title="Evaluation" icon="check-double" href="/evaluation">
    Deep dive into evaluation methods.
  </Card>
</CardGroup>
